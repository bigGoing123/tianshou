{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76fbdf37447a0cfd",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-22T12:31:05.732195100Z",
     "start_time": "2024-02-22T12:31:05.728184100Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from gymnasium.spaces import Box\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import ActorCritic, DataParallelNet, Net\n",
    "from tianshou.utils.net.discrete import Actor, Critic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--task\", type=str, default=\"LunarLander-v2\")\n",
    "    parser.add_argument(\"--reward-threshold\", type=float, default=None)\n",
    "    parser.add_argument(\"--seed\", type=int, default=1626)\n",
    "    parser.add_argument(\"--buffer-size\", type=int, default=20000)\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--epoch\", type=int, default=10)\n",
    "    parser.add_argument(\"--step-per-epoch\", type=int, default=50000)\n",
    "    parser.add_argument(\"--step-per-collect\", type=int, default=2000)\n",
    "    parser.add_argument(\"--repeat-per-collect\", type=int, default=10)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=64)\n",
    "    parser.add_argument(\"--hidden-sizes\", type=int, nargs=\"*\", default=[64, 64])\n",
    "    parser.add_argument(\"--training-num\", type=int, default=20)\n",
    "    parser.add_argument(\"--test-num\", type=int, default=100)\n",
    "    parser.add_argument(\"--logdir\", type=str, default=\"log\")\n",
    "    parser.add_argument(\"--render\", type=float, default=0.0)\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        type=str,\n",
    "        default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    # ppo special\n",
    "    parser.add_argument(\"--vf-coef\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--ent-coef\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--eps-clip\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--gae-lambda\", type=float, default=0.95)\n",
    "    parser.add_argument(\"--rew-norm\", type=int, default=0)\n",
    "    parser.add_argument(\"--norm-adv\", type=int, default=0)\n",
    "    parser.add_argument(\"--recompute-adv\", type=int, default=0)\n",
    "    parser.add_argument(\"--dual-clip\", type=float, default=None)\n",
    "    parser.add_argument(\"--value-clip\", type=int, default=0)\n",
    "    return parser.parse_known_args()[0]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T12:31:05.739100600Z",
     "start_time": "2024-02-22T12:31:05.737102Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 50001it [00:40, 1227.01it/s, env_step=50000, gradient_step=800, len=180, n/ep=1, n/st=2000, rew=-60.82]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -252.066959 ± 26.117025, best_reward: -162.349102 ± 135.970389 in #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 50001it [00:41, 1213.69it/s, env_step=100000, gradient_step=1600, len=1000, n/ep=2, n/st=2000, rew=57.09]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -3.640651 ± 40.743170, best_reward: -3.640651 ± 40.743170 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 50001it [00:48, 1031.11it/s, env_step=150000, gradient_step=2400, len=629, n/ep=2, n/st=2000, rew=12.36]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 75.231847 ± 84.574756, best_reward: 75.231847 ± 84.574756 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 50001it [00:49, 1015.98it/s, env_step=200000, gradient_step=3200, len=873, n/ep=4, n/st=2000, rew=97.47]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: -53.330257 ± 23.724646, best_reward: 75.231847 ± 84.574756 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 50001it [00:40, 1236.36it/s, env_step=250000, gradient_step=4000, len=973, n/ep=2, n/st=2000, rew=132.50]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: -62.996039 ± 84.763657, best_reward: 75.231847 ± 84.574756 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 50001it [00:39, 1251.45it/s, env_step=300000, gradient_step=4800, len=651, n/ep=0, n/st=2000, rew=16.79]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 11.072115 ± 89.324507, best_reward: 75.231847 ± 84.574756 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 50001it [00:47, 1054.88it/s, env_step=350000, gradient_step=5600, len=1000, n/ep=3, n/st=2000, rew=97.85]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 69.825404 ± 102.039539, best_reward: 75.231847 ± 84.574756 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 50001it [00:40, 1238.86it/s, env_step=400000, gradient_step=6400, len=690, n/ep=4, n/st=2000, rew=91.85]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: -4.331552 ± 111.778525, best_reward: 75.231847 ± 84.574756 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 50001it [00:40, 1236.03it/s, env_step=450000, gradient_step=7200, len=654, n/ep=2, n/st=2000, rew=141.86]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 51.896893 ± 119.259292, best_reward: 75.231847 ± 84.574756 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 50001it [00:45, 1096.20it/s, env_step=500000, gradient_step=8000, len=419, n/ep=5, n/st=2000, rew=97.42]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: -70.558810 ± 226.807568, best_reward: 75.231847 ± 84.574756 in #3\n"
     ]
    }
   ],
   "source": [
    "args=get_args()\n",
    "env = gym.make(args.task)\n",
    "args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "args.action_shape = env.action_space.shape or env.action_space.n\n",
    "if args.reward_threshold is None:\n",
    "    default_reward_threshold = {\"CartPole-v0\": 195}\n",
    "    args.reward_threshold = default_reward_threshold.get(args.task, env.spec.reward_threshold)\n",
    "# train_envs = gym.make(args.task)\n",
    "# you can also use tianshou.env.SubprocVectorEnv\n",
    "train_envs = DummyVectorEnv([lambda: gym.make(args.task) for _ in range(args.training_num)])\n",
    "# test_envs = gym.make(args.task)\n",
    "test_envs = DummyVectorEnv([lambda: gym.make(args.task) for _ in range(args.test_num)])\n",
    "# seed\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "train_envs.seed(args.seed)\n",
    "test_envs.seed(args.seed)\n",
    "# model\n",
    "net = Net(args.state_shape, hidden_sizes=args.hidden_sizes, device=args.device)\n",
    "if torch.cuda.is_available():\n",
    "    actor = DataParallelNet(Actor(net, args.action_shape, device=None).to(args.device))\n",
    "    critic = DataParallelNet(Critic(net, device=None).to(args.device))\n",
    "else:\n",
    "    actor = Actor(net, args.action_shape, device=args.device).to(args.device)\n",
    "    critic = Critic(net, device=args.device).to(args.device)\n",
    "actor_critic = ActorCritic(actor, critic)\n",
    "# orthogonal initialization\n",
    "for m in actor_critic.modules():\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.orthogonal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "optim = torch.optim.Adam(actor_critic.parameters(), lr=args.lr)\n",
    "dist = torch.distributions.Categorical\n",
    "policy = PPOPolicy(\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    optim=optim,\n",
    "    dist_fn=dist,\n",
    "    action_scaling=isinstance(env.action_space, Box),\n",
    "    discount_factor=args.gamma,\n",
    "    max_grad_norm=args.max_grad_norm,\n",
    "    eps_clip=args.eps_clip,\n",
    "    vf_coef=args.vf_coef,\n",
    "    ent_coef=args.ent_coef,\n",
    "    gae_lambda=args.gae_lambda,\n",
    "    reward_normalization=args.rew_norm,\n",
    "    dual_clip=args.dual_clip,\n",
    "    value_clip=args.value_clip,\n",
    "    action_space=env.action_space,\n",
    "    deterministic_eval=True,\n",
    "    advantage_normalization=args.norm_adv,\n",
    "    recompute_advantage=args.recompute_adv,\n",
    ")\n",
    "# collector\n",
    "train_collector = Collector(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    VectorReplayBuffer(args.buffer_size, len(train_envs)),\n",
    ")\n",
    "test_collector = Collector(policy, test_envs)\n",
    "# log\n",
    "log_path = os.path.join(args.logdir, args.task, \"ppo\")\n",
    "writer = SummaryWriter(log_path)\n",
    "logger = TensorboardLogger(writer)\n",
    "\n",
    "def save_best_fn(policy):\n",
    "    torch.save(policy.state_dict(), os.path.join(log_path, \"policy.pth\"))\n",
    "\n",
    "def stop_fn(mean_rewards):\n",
    "    return mean_rewards >= args.reward_threshold\n",
    "\n",
    "# trainer\n",
    "result = OnpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=args.epoch,\n",
    "    step_per_epoch=args.step_per_epoch,\n",
    "    repeat_per_collect=args.repeat_per_collect,\n",
    "    episode_per_test=args.test_num,\n",
    "    batch_size=args.batch_size,\n",
    "    step_per_collect=args.step_per_collect,\n",
    "    stop_fn=stop_fn,\n",
    "    save_best_fn=save_best_fn,\n",
    "    logger=logger,\n",
    ").run()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T12:39:24.847043200Z",
     "start_time": "2024-02-22T12:31:05.746100800Z"
    }
   },
   "id": "a7e2d2ff0bfad284"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final reward: -115.63431414646574, length: 866.0\n",
      "Final reward: -39.589728014548726, length: 475.0\n",
      "Final reward: -201.59155501379962, length: 478.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's watch its performance!\n",
    "env = gym.make(args.task,render_mode=\"human\")\n",
    "num=3\n",
    "policy.eval()\n",
    "for _ in range(num):\n",
    "    \n",
    "    collector = Collector(policy, env,exploration_noise=True)\n",
    "    result = collector.collect(n_episode=1, render=args.render)\n",
    "    print(f\"Final reward: {result.returns_stat.mean}, length: {result.lens_stat.mean}\")\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T12:40:02.727447300Z",
     "start_time": "2024-02-22T12:39:24.850044500Z"
    }
   },
   "id": "ff7d45adf9450f91"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T12:40:02.730446200Z",
     "start_time": "2024-02-22T12:40:02.727447300Z"
    }
   },
   "id": "39f09b81bdf12a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
