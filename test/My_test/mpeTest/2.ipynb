{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import rl_utils"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T12:01:49.301940300Z",
     "start_time": "2024-03-07T12:01:46.233090900Z"
    }
   },
   "id": "954c45d1f8f40129"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "def onehot_from_logits(logits, eps=0.01):\n",
    "    ''' 生成最优动作的独热（one-hot）形式 '''\n",
    "    logits = logits.view(1, -1)  # 将一维张量转为二维\n",
    "    argmax_acs = (logits == logits.max(1, keepdim=True)[0]).float()\n",
    "    # 生成随机动作,转换成独热形式\n",
    "    rand_acs = torch.autograd.Variable(torch.eye(logits.shape[1])[[\n",
    "        np.random.choice(range(logits.shape[1]), size=logits.shape[0])\n",
    "    ]],\n",
    "                                       requires_grad=False).to(logits.device)\n",
    "    # 通过epsilon-贪婪算法来选择用哪个动作\n",
    "    return torch.stack([\n",
    "        argmax_acs[i] if r > eps else rand_acs[i]\n",
    "        for i, r in enumerate(torch.rand(logits.shape[0]))\n",
    "    ])\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20, tens_type=torch.FloatTensor):\n",
    "    \"\"\"从Gumbel(0,1)分布中采样\"\"\"\n",
    "    U = torch.autograd.Variable(tens_type(*shape).uniform_(),\n",
    "                                requires_grad=False)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    \"\"\" 从Gumbel-Softmax分布中采样\"\"\"\n",
    "    y = logits + sample_gumbel(logits.shape, tens_type=type(logits.data)).to(\n",
    "        logits.device)\n",
    "    return F.softmax(y / temperature, dim=0)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1.0):\n",
    "    \"\"\"从Gumbel-Softmax分布中采样,并进行离散化\"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    y_hard = onehot_from_logits(y)\n",
    "    y = (y_hard.to(logits.device) - y).detach() + y\n",
    "    # 返回一个y_hard的独热量,但是它的梯度是y,我们既能够得到一个与环境交互的离散动作,又可以\n",
    "    # 正确地反传梯度\n",
    "    return y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T12:01:49.310568100Z",
     "start_time": "2024-03-07T12:01:49.302939700Z"
    }
   },
   "id": "ba9c41f97c87d130"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class TwoLayerFC(torch.nn.Module):\n",
    "    def __init__(self, num_in, num_out, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_in, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T12:01:49.310568100Z",
     "start_time": "2024-03-07T12:01:49.308180100Z"
    }
   },
   "id": "4af2b7427beab3b3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\n",
    "class DDPG:\n",
    "    ''' DDPG算法 '''\n",
    "    def __init__(self, state_dim, action_dim, critic_input_dim, hidden_dim,\n",
    "                 actor_lr, critic_lr, device):\n",
    "        self.actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_actor = TwoLayerFC(state_dim, action_dim,\n",
    "                                       hidden_dim).to(device)\n",
    "        self.critic = TwoLayerFC(critic_input_dim, 1, hidden_dim).to(device)\n",
    "        self.target_critic = TwoLayerFC(critic_input_dim, 1,\n",
    "                                        hidden_dim).to(device)\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n",
    "                                                 lr=critic_lr)\n",
    "\n",
    "    def take_action(self, state, explore=False):\n",
    "        action = self.actor(state)\n",
    "        if explore:\n",
    "            action = gumbel_softmax(action)\n",
    "        else:\n",
    "            action = onehot_from_logits(action)\n",
    "        return action.detach().cpu().numpy()[0].argmax()\n",
    "\n",
    "    def soft_update(self, net, target_net, tau):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) +\n",
    "                                    param.data * tau)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T12:01:49.323911800Z",
     "start_time": "2024-03-07T12:01:49.310568100Z"
    }
   },
   "id": "303eaa4c8e5a01bf"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, env, device, actor_lr, critic_lr, hidden_dim,\n",
    "                 state_dims, action_dims, critic_input_dim, gamma, tau):\n",
    "        self.agents = []\n",
    "        for i in range(len(env.agents)):\n",
    "            self.agents.append(\n",
    "                DDPG(state_dims[i], action_dims[i], critic_input_dim,\n",
    "                     hidden_dim, actor_lr, critic_lr, device))\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.critic_criterion = torch.nn.MSELoss()\n",
    "        self.device = device\n",
    "\n",
    "    @property\n",
    "    def policies(self):\n",
    "        return [agt.actor for agt in self.agents]\n",
    "\n",
    "    @property\n",
    "    def target_policies(self):\n",
    "        return [agt.target_actor for agt in self.agents]\n",
    "\n",
    "    def take_action(self, states, explore):\n",
    "        #np.cumsum(state_dims)[:-1] 计算了 state_dims 的累积和，然后通过 np.split 根据这些分割点进行分割，得到了一个包含4个子列表的列表\n",
    "        reorganized_states = np.split(states, np.cumsum(state_dims)[:-1])\n",
    "        # print(f\"reorganized_states{reorganized_states}\")\n",
    "        #重新将states 转化为tensor\n",
    "        states = [torch.tensor(sublist,dtype=torch.float32,device=self.device) for sublist in reorganized_states]\n",
    "        return [\n",
    "            agent.take_action(state, explore)\n",
    "            for agent, state in zip(self.agents, states)\n",
    "        ]\n",
    "\n",
    "    def update(self, sample, i_agent):\n",
    "        obs, act, rew, next_obs, done = sample\n",
    "        cur_agent = self.agents[i_agent]\n",
    "\n",
    "        cur_agent.critic_optimizer.zero_grad()\n",
    "        all_target_act = [\n",
    "            onehot_from_logits(pi(_next_obs))\n",
    "            for pi, _next_obs in zip(self.target_policies, next_obs)\n",
    "        ]\n",
    "        target_critic_input = torch.cat((*next_obs, *all_target_act), dim=1)\n",
    "        target_critic_value = rew[i_agent].view(\n",
    "            -1, 1) + self.gamma * cur_agent.target_critic(\n",
    "                target_critic_input) * (1 - done[i_agent].view(-1, 1))\n",
    "        critic_input = torch.cat((*obs, *act), dim=1)\n",
    "        critic_value = cur_agent.critic(critic_input)\n",
    "        critic_loss = self.critic_criterion(critic_value,\n",
    "                                            target_critic_value.detach())\n",
    "        critic_loss.backward()\n",
    "        cur_agent.critic_optimizer.step()\n",
    "\n",
    "        cur_agent.actor_optimizer.zero_grad()\n",
    "        cur_actor_out = cur_agent.actor(obs[i_agent])\n",
    "        cur_act_vf_in = gumbel_softmax(cur_actor_out)\n",
    "        all_actor_acs = []\n",
    "        for i, (pi, _obs) in enumerate(zip(self.policies, obs)):\n",
    "            if i == i_agent:\n",
    "                all_actor_acs.append(cur_act_vf_in)\n",
    "            else:\n",
    "                all_actor_acs.append(onehot_from_logits(pi(_obs)))\n",
    "        vf_in = torch.cat((*obs, *all_actor_acs), dim=1)\n",
    "        actor_loss = -cur_agent.critic(vf_in).mean()\n",
    "        actor_loss += (cur_actor_out**2).mean() * 1e-3\n",
    "        actor_loss.backward()\n",
    "        cur_agent.actor_optimizer.step()\n",
    "\n",
    "    def update_all_targets(self):\n",
    "        for agt in self.agents:\n",
    "            agt.soft_update(agt.actor, agt.target_actor, self.tau)\n",
    "            agt.soft_update(agt.critic, agt.target_critic, self.tau)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T12:01:49.329806400Z",
     "start_time": "2024-03-07T12:01:49.323911800Z"
    }
   },
   "id": "9a17e0f6031b5aac"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20230921a\\AppData\\Roaming\\Python\\Python311\\site-packages\\pettingzoo\\utils\\conversions.py:158: UserWarning: The `action_spaces` dictionary is deprecated. Use the `action_space` function instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\20230921a\\AppData\\Roaming\\Python\\Python311\\site-packages\\pettingzoo\\utils\\conversions.py:144: UserWarning: The `observation_spaces` dictionary is deprecated. Use the `observation_space` function instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "num_episodes = 5000\n",
    "episode_length = 25  # 每条序列的最大长度\n",
    "buffer_size = 100000\n",
    "hidden_dim = 64\n",
    "actor_lr = 1e-2\n",
    "critic_lr = 1e-2\n",
    "gamma = 0.95\n",
    "tau = 1e-2\n",
    "batch_size = 1024\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "update_interval = 100\n",
    "minimal_size = 4000\n",
    "\n",
    "env_id = \"simple_adversary\"\n",
    "env = simple_adversary_v3.parallel_env(render_mode=\"human\")\n",
    "env.reset()\n",
    "replay_buffer = rl_utils.ReplayBuffer(buffer_size)\n",
    "\n",
    "state_dims = []\n",
    "action_dims = []\n",
    "for action_space in env.action_spaces.values():\n",
    "    action_dims.append(action_space.n)\n",
    "for state_space in env.observation_spaces.values():\n",
    "    state_dims.append(state_space.shape[0])\n",
    "critic_input_dim = np.sum(state_dims) + np.sum(action_dims)\n",
    "maddpg = MADDPG(env, device, actor_lr, critic_lr, hidden_dim, state_dims,\n",
    "                action_dims, critic_input_dim, gamma, tau)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T12:01:50.326098900Z",
     "start_time": "2024-03-07T12:01:49.329806400Z"
    }
   },
   "id": "4da141aa12c30e37"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def evaluate( maddpg, n_episode=10, episode_length=25):\n",
    "    # 对学习的策略进行评估,此时不会进行探索\n",
    "    env = simple_adversary_v3.parallel_env(render_mode=\"rgb_array\")\n",
    "    env.reset()\n",
    "    returns = np.zeros(len(env.agents))\n",
    "    for _ in range(n_episode):\n",
    "        sum=0\n",
    "        \n",
    "        for t_i in range(episode_length):\n",
    "            actions = maddpg.take_action(env.state(), explore=False)\n",
    "            actions = dict(zip(env.agents, actions))\n",
    "            observation, reward, _, _, _ = env.step(actions)\n",
    "            # print(len(reward.values()))\n",
    "            # sum+=list(reward.values())[1]\n",
    "            returns += np.array(list(reward.values()))/n_episode   \n",
    "        \n",
    "        env.reset()\n",
    "    env.close()\n",
    "    return returns.tolist()\n",
    "\n",
    "\n",
    "return_list = []  # 记录每一轮的回报（return）\n",
    "total_step = 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T12:01:50.331183500Z",
     "start_time": "2024-03-07T12:01:50.327098100Z"
    }
   },
   "id": "99f8342c86a8dc94"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, [-47.7386645143229, 30.8076377736785, 30.8076377736785, 30.8076377736785, 30.8076377736785, 30.8076377736785]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31merror\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 9\u001B[0m\n\u001B[0;32m      7\u001B[0m actions \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(env\u001B[38;5;241m.\u001B[39magents, actions))\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# print(total_step,actions)\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m observation, reward, termination, truncation, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(actions)\n\u001B[0;32m     10\u001B[0m done \u001B[38;5;241m=\u001B[39m termination \u001B[38;5;129;01mor\u001B[39;00m truncation\n\u001B[0;32m     12\u001B[0m replay_buffer\u001B[38;5;241m.\u001B[39madd(env\u001B[38;5;241m.\u001B[39mstate(), actions, reward, observation, done)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pettingzoo\\utils\\conversions.py:207\u001B[0m, in \u001B[0;36maec_to_parallel_wrapper.step\u001B[1;34m(self, actions)\u001B[0m\n\u001B[0;32m    203\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    204\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected agent \u001B[39m\u001B[38;5;132;01m{\u001B[39;00magent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m got agent \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maec_env\u001B[38;5;241m.\u001B[39magent_selection\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Parallel environment wrapper expects agents to step in a cycle.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    205\u001B[0m         )\n\u001B[0;32m    206\u001B[0m obs, rew, termination, truncation, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maec_env\u001B[38;5;241m.\u001B[39mlast()\n\u001B[1;32m--> 207\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maec_env\u001B[38;5;241m.\u001B[39mstep(actions[agent])\n\u001B[0;32m    208\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m agent \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maec_env\u001B[38;5;241m.\u001B[39magents:\n\u001B[0;32m    209\u001B[0m     rewards[agent] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maec_env\u001B[38;5;241m.\u001B[39mrewards[agent]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pettingzoo\\utils\\wrappers\\order_enforcing.py:91\u001B[0m, in \u001B[0;36mOrderEnforcingWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     90\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_updated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 91\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:116\u001B[0m, in \u001B[0;36mBaseWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action: ActionType) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent_selection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39magent_selection\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mrewards\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pettingzoo\\utils\\wrappers\\assert_out_of_bounds.py:26\u001B[0m, in \u001B[0;36mAssertOutOfBoundsWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action: ActionType) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[0;32m     18\u001B[0m         action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     19\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     24\u001B[0m         action\n\u001B[0;32m     25\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maction is not in action space\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 26\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:116\u001B[0m, in \u001B[0;36mBaseWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action: ActionType) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent_selection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39magent_selection\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrewards \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mrewards\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pettingzoo\\mpe\\_mpe_utils\\simple_env.py:263\u001B[0m, in \u001B[0;36mSimpleEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accumulate_rewards()\n\u001B[0;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 263\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender()\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pettingzoo\\mpe\\_mpe_utils\\simple_env.py:279\u001B[0m, in \u001B[0;36mSimpleEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    275\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menable_render(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode)\n\u001B[1;32m--> 279\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdraw()\n\u001B[0;32m    280\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrgb_array\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    281\u001B[0m     observation \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(pygame\u001B[38;5;241m.\u001B[39msurfarray\u001B[38;5;241m.\u001B[39mpixels3d(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscreen))\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pettingzoo\\mpe\\_mpe_utils\\simple_env.py:289\u001B[0m, in \u001B[0;36mSimpleEnv.draw\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdraw\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    288\u001B[0m     \u001B[38;5;66;03m# clear screen\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscreen\u001B[38;5;241m.\u001B[39mfill((\u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m255\u001B[39m))\n\u001B[0;32m    291\u001B[0m     \u001B[38;5;66;03m# update bounds to center around agent\u001B[39;00m\n\u001B[0;32m    292\u001B[0m     all_poses \u001B[38;5;241m=\u001B[39m [entity\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mp_pos \u001B[38;5;28;01mfor\u001B[39;00m entity \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworld\u001B[38;5;241m.\u001B[39mentities]\n",
      "\u001B[1;31merror\u001B[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    # ep_returns = np.zeros(len(env.agents))\n",
    "    # for e_i in range(episode_length):\n",
    "    while env.agents:\n",
    "        actions = maddpg.take_action(env.state(), explore=True)\n",
    "        actions = dict(zip(env.agents, actions))\n",
    "        # print(total_step,actions)\n",
    "        observation, reward, termination, truncation, info = env.step(actions)\n",
    "        done = termination or truncation\n",
    "\n",
    "        replay_buffer.add(env.state(), actions, reward, observation, done)\n",
    "\n",
    "        total_step += 1\n",
    "        if replay_buffer.size() >= minimal_size and total_step % update_interval == 0:\n",
    "            sample = replay_buffer.sample(batch_size=1)\n",
    "            # def stack_array(x):\n",
    "            #     rearranged = [[sub_x[i] for sub_x in x]\n",
    "            #                   for i in range(len(x[0]))]\n",
    "            #     return [\n",
    "            #         torch.FloatTensor(np.vstack(aa)).to(device)\n",
    "            #         for aa in rearranged\n",
    "            #     ]\n",
    "            # \n",
    "            # print(sample)\n",
    "            # sample = [stack_array(x) for x in sample]\n",
    "            for a_i in range(len(env.agents)):\n",
    "                maddpg.update(sample, a_i)\n",
    "            maddpg.update_all_targets()\n",
    "    \n",
    "    if (i_episode + 1) % 100 == 0:\n",
    "        ep_returns = evaluate(maddpg, n_episode=100)\n",
    "        return_list.append(ep_returns)\n",
    "        print(f\"Episode: {i_episode + 1}, {ep_returns}\")\n",
    "    # env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-07T12:02:21.260903200Z",
     "start_time": "2024-03-07T12:01:50.334183500Z"
    }
   },
   "id": "4f84d1279b354d00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "\n",
    "env = simple_adversary_v3.parallel_env(render_mode=\"human\")\n",
    "observations, infos = env.reset()\n",
    "print(len(env.state()))\n",
    "sum=0\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "    print(actions)\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    sum+=1\n",
    "    if sum>10:\n",
    "        break\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-07T12:02:21.258903600Z"
    }
   },
   "id": "9a5e828c20af24a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "\n",
    "env = simple_adversary_v3.parallel_env(render_mode=\"human\")\n",
    "observations, infos = env.reset()\n",
    "sum=0\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    sum+=1\n",
    "    print(f\"terminations:{terminations},truncations:{truncations}\")\n",
    "print(sum)\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-07T12:02:21.260903200Z"
    }
   },
   "id": "3f6144dc5a3b716"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
