{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76fbdf37447a0cfd",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-22T03:10:46.907209Z",
     "start_time": "2024-02-22T03:10:46.895318Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from gymnasium.spaces import Box\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.policy import ICMPolicy, PPOPolicy\n",
    "from tianshou.trainer import OnpolicyTrainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import MLP, ActorCritic, Net\n",
    "from tianshou.utils.net.discrete import Actor, Critic, IntrinsicCuriosityModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--task\", type=str, default=\"LunarLander-v2\")\n",
    "    parser.add_argument(\"--reward-threshold\", type=float, default=200)\n",
    "    parser.add_argument(\"--seed\", type=int, default=1626)\n",
    "    parser.add_argument(\"--buffer-size\", type=int, default=20000)\n",
    "    parser.add_argument(\"--lr\", type=float, default=3e-4)\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99)\n",
    "    parser.add_argument(\"--epoch\", type=int, default=10)\n",
    "    parser.add_argument(\"--step-per-epoch\", type=int, default=50000)\n",
    "    parser.add_argument(\"--step-per-collect\", type=int, default=2000)\n",
    "    parser.add_argument(\"--repeat-per-collect\", type=int, default=10)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=64)\n",
    "    parser.add_argument(\"--hidden-sizes\", type=int, nargs=\"*\", default=[64, 64])\n",
    "    parser.add_argument(\"--training-num\", type=int, default=20)\n",
    "    parser.add_argument(\"--test-num\", type=int, default=100)\n",
    "    parser.add_argument(\"--logdir\", type=str, default=\"log\")\n",
    "    parser.add_argument(\"--render\", type=float, default=0.0)\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        type=str,\n",
    "        default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    # ppo special\n",
    "    parser.add_argument(\"--vf-coef\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--ent-coef\", type=float, default=0.0)\n",
    "    parser.add_argument(\"--eps-clip\", type=float, default=0.2)\n",
    "    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5)\n",
    "    parser.add_argument(\"--gae-lambda\", type=float, default=0.95)\n",
    "    parser.add_argument(\"--rew-norm\", type=int, default=0)\n",
    "    parser.add_argument(\"--norm-adv\", type=int, default=0)\n",
    "    parser.add_argument(\"--recompute-adv\", type=int, default=0)\n",
    "    parser.add_argument(\"--dual-clip\", type=float, default=None)\n",
    "    parser.add_argument(\"--value-clip\", type=int, default=0)\n",
    "    parser.add_argument(\n",
    "        \"--lr-scale\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"use intrinsic curiosity module with this lr scale\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--reward-scale\",\n",
    "        type=float,\n",
    "        default=0.01,\n",
    "        help=\"scaling factor for intrinsic curiosity reward\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--forward-loss-weight\",\n",
    "        type=float,\n",
    "        default=0.2,\n",
    "        help=\"weight for the forward model loss in ICM\",\n",
    "    )\n",
    "    return parser.parse_known_args()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T03:10:46.908209300Z",
     "start_time": "2024-02-22T03:10:46.904698Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "args=get_args()\n",
    "env = gym.make(args.task,render_mode=\"human\")\n",
    "args.state_shape = env.observation_space.shape or env.observation_space.n\n",
    "args.action_shape = env.action_space.shape or env.action_space.n\n",
    "if args.reward_threshold is None:\n",
    "    default_reward_threshold = {\"CartPole-v0\": 200}\n",
    "    args.reward_threshold = default_reward_threshold.get(args.task, env.spec.reward_threshold)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T03:10:46.913211700Z",
     "start_time": "2024-02-22T03:10:46.908209300Z"
    }
   },
   "id": "b6262e8bd2279c4a"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# train_envs = gym.make(args.task)\n",
    "# you can also use tianshou.env.SubprocVectorEnv\n",
    "train_envs = DummyVectorEnv([lambda: gym.make(args.task) for _ in range(args.training_num)])\n",
    "# test_envs = gym.make(args.task)\n",
    "test_envs = DummyVectorEnv([lambda: gym.make(args.task) for _ in range(args.test_num)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T03:10:46.951822100Z",
     "start_time": "2024-02-22T03:10:46.916224200Z"
    }
   },
   "id": "61341b7d68598652"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "[[1626],\n [1627],\n [1628],\n [1629],\n [1630],\n [1631],\n [1632],\n [1633],\n [1634],\n [1635],\n [1636],\n [1637],\n [1638],\n [1639],\n [1640],\n [1641],\n [1642],\n [1643],\n [1644],\n [1645],\n [1646],\n [1647],\n [1648],\n [1649],\n [1650],\n [1651],\n [1652],\n [1653],\n [1654],\n [1655],\n [1656],\n [1657],\n [1658],\n [1659],\n [1660],\n [1661],\n [1662],\n [1663],\n [1664],\n [1665],\n [1666],\n [1667],\n [1668],\n [1669],\n [1670],\n [1671],\n [1672],\n [1673],\n [1674],\n [1675],\n [1676],\n [1677],\n [1678],\n [1679],\n [1680],\n [1681],\n [1682],\n [1683],\n [1684],\n [1685],\n [1686],\n [1687],\n [1688],\n [1689],\n [1690],\n [1691],\n [1692],\n [1693],\n [1694],\n [1695],\n [1696],\n [1697],\n [1698],\n [1699],\n [1700],\n [1701],\n [1702],\n [1703],\n [1704],\n [1705],\n [1706],\n [1707],\n [1708],\n [1709],\n [1710],\n [1711],\n [1712],\n [1713],\n [1714],\n [1715],\n [1716],\n [1717],\n [1718],\n [1719],\n [1720],\n [1721],\n [1722],\n [1723],\n [1724],\n [1725]]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# seed\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "train_envs.seed(args.seed)\n",
    "test_envs.seed(args.seed)\n",
    "# model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T03:10:47.001019700Z",
     "start_time": "2024-02-22T03:10:46.953820500Z"
    }
   },
   "id": "1fba2d26be7083df"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "\n",
    "net = Net(args.state_shape, hidden_sizes=args.hidden_sizes, device=args.device)\n",
    "actor = Actor(net, args.action_shape, device=args.device).to(args.device)\n",
    "critic = Critic(net, device=args.device).to(args.device)\n",
    "actor_critic = ActorCritic(actor, critic)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T03:10:47.004946200Z",
     "start_time": "2024-02-22T03:10:47.001019700Z"
    }
   },
   "id": "df8f75c0a3f92c57"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "\n",
    "# orthogonal initialization\n",
    "for m in actor_critic.modules():\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.orthogonal_(m.weight)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "optim = torch.optim.Adam(actor_critic.parameters(), lr=args.lr)\n",
    "dist = torch.distributions.Categorical\n",
    "policy = PPOPolicy(\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    optim=optim,\n",
    "    dist_fn=dist,\n",
    "    action_scaling=isinstance(env.action_space, Box),\n",
    "    discount_factor=args.gamma,\n",
    "    max_grad_norm=args.max_grad_norm,\n",
    "    eps_clip=args.eps_clip,\n",
    "    vf_coef=args.vf_coef,\n",
    "    ent_coef=args.ent_coef,\n",
    "    gae_lambda=args.gae_lambda,\n",
    "    reward_normalization=args.rew_norm,\n",
    "    dual_clip=args.dual_clip,\n",
    "    value_clip=args.value_clip,\n",
    "    action_space=env.action_space,\n",
    "    deterministic_eval=True,\n",
    "    advantage_normalization=args.norm_adv,\n",
    "    recompute_advantage=args.recompute_adv,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T03:10:47.010445100Z",
     "start_time": "2024-02-22T03:10:47.007947200Z"
    }
   },
   "id": "48d53dcf8698bb68"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "\n",
    "feature_dim = args.hidden_sizes[-1]\n",
    "feature_net = MLP(\n",
    "    np.prod(args.state_shape),\n",
    "    output_dim=feature_dim,\n",
    "    hidden_sizes=args.hidden_sizes[:-1],\n",
    "    device=args.device,\n",
    ")\n",
    "action_dim = np.prod(args.action_shape)\n",
    "icm_net = IntrinsicCuriosityModule(\n",
    "    feature_net,\n",
    "    feature_dim,\n",
    "    action_dim,\n",
    "    hidden_sizes=args.hidden_sizes[-1:],\n",
    "    device=args.device,\n",
    ").to(args.device)\n",
    "icm_optim = torch.optim.Adam(icm_net.parameters(), lr=args.lr)\n",
    "policy = ICMPolicy(\n",
    "    policy=policy,\n",
    "    model=icm_net,\n",
    "    optim=icm_optim,\n",
    "    action_space=env.action_space,\n",
    "    lr_scale=args.lr_scale,\n",
    "    reward_scale=args.reward_scale,\n",
    "    forward_loss_weight=args.forward_loss_weight,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T03:10:47.018443500Z",
     "start_time": "2024-02-22T03:10:47.014443500Z"
    }
   },
   "id": "97e83844d46a6aeb"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "\n",
    "# collector\n",
    "train_collector = Collector(\n",
    "    policy,\n",
    "    train_envs,\n",
    "    VectorReplayBuffer(args.buffer_size, len(train_envs)),\n",
    ")\n",
    "test_collector = Collector(policy, test_envs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T03:10:47.053193600Z",
     "start_time": "2024-02-22T03:10:47.026443900Z"
    }
   },
   "id": "148b23a39ccfbb81"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 50001it [00:21, 2323.57it/s, env_step=50000, gradient_step=800, len=216, n/ep=0, n/st=2000, rew=-61.36]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -173.421078 ± 39.062908, best_reward: -173.421078 ± 39.062908 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 50001it [00:21, 2300.87it/s, env_step=100000, gradient_step=1600, len=770, n/ep=0, n/st=2000, rew=41.51]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: -25.458615 ± 20.297596, best_reward: -25.458615 ± 20.297596 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 50001it [00:22, 2222.29it/s, env_step=150000, gradient_step=2400, len=677, n/ep=4, n/st=2000, rew=78.27]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: -3.289702 ± 25.646749, best_reward: -3.289702 ± 25.646749 in #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 50001it [00:22, 2189.96it/s, env_step=200000, gradient_step=3200, len=385, n/ep=5, n/st=2000, rew=164.02]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 100.161273 ± 146.044257, best_reward: 100.161273 ± 146.044257 in #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 50001it [00:22, 2210.19it/s, env_step=250000, gradient_step=4000, len=332, n/ep=6, n/st=2000, rew=179.46]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 168.078289 ± 132.446801, best_reward: 168.078289 ± 132.446801 in #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 50001it [00:22, 2232.51it/s, env_step=300000, gradient_step=4800, len=405, n/ep=6, n/st=2000, rew=239.54]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 262.878976 ± 35.652734, best_reward: 262.878976 ± 35.652734 in #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 50001it [00:21, 2290.57it/s, env_step=350000, gradient_step=5600, len=249, n/ep=2, n/st=2000, rew=246.64]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 256.336271 ± 64.157217, best_reward: 262.878976 ± 35.652734 in #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 50001it [00:21, 2322.56it/s, env_step=400000, gradient_step=6400, len=207, n/ep=5, n/st=2000, rew=220.47]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 259.509690 ± 41.484792, best_reward: 262.878976 ± 35.652734 in #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 50001it [00:21, 2318.57it/s, env_step=450000, gradient_step=7200, len=223, n/ep=8, n/st=2000, rew=254.75]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 275.918579 ± 16.502602, best_reward: 275.918579 ± 16.502602 in #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 50001it [00:23, 2140.37it/s, env_step=500000, gradient_step=8000, len=225, n/ep=6, n/st=2000, rew=236.83]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 235.407737 ± 80.287550, best_reward: 275.918579 ± 16.502602 in #9\n"
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "result = OnpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=args.epoch,\n",
    "    step_per_epoch=args.step_per_epoch,\n",
    "    repeat_per_collect=args.repeat_per_collect,\n",
    "    episode_per_test=args.test_num,\n",
    "    batch_size=args.batch_size,\n",
    "    step_per_collect=args.step_per_collect,\n",
    ").run()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T03:15:11.021555800Z",
     "start_time": "2024-02-22T03:10:47.077192500Z"
    }
   },
   "id": "f652a163fe4b9736"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final reward: 257.96853752617494, length: 255.0\n",
      "Final reward: 51.20673710968089, length: 1000.0\n",
      "Final reward: 243.51113859610987, length: 260.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(args.task,render_mode=\"human\")\n",
    "policy.eval()\n",
    "test_num=3\n",
    "for _ in range(test_num):\n",
    "    collector = Collector(policy, env)\n",
    "    result = collector.collect(n_episode=1, render=args.render)\n",
    "    print(f\"Final reward: {result.returns_stat.mean}, length: {result.lens_stat.mean}\")\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T03:17:47.833331200Z",
     "start_time": "2024-02-22T03:17:16.273092900Z"
    }
   },
   "id": "5d460af0e919cfea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
