{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-06T08:34:58.717477700Z",
     "start_time": "2024-03-06T08:34:58.710274Z"
    }
   },
   "outputs": [],
   "source": [
    "#导入相关包\n",
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "from typing import Tuple, Any\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from pettingzoo.mpe import simple_adversary_v3\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from tianshou.data import Collector, VectorReplayBuffer, InfoStats\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import BasePolicy, DQNPolicy, MultiAgentPolicyManager, RandomPolicy\n",
    "from tianshou.trainer import OffpolicyTrainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "outputs": [],
   "source": [
    "#设置参数\n",
    "\n",
    "def get_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--seed\", type=int, default=1626)#设置随机种子\n",
    "    parser.add_argument(\"--eps-test\", type=float, default=0.05)#测试时的epsilon\n",
    "    parser.add_argument(\"--eps-train\", type=float, default=0.1)#训练时的epsilon\n",
    "    parser.add_argument(\"--buffer-size\", type=int, default=20000)#经验回放缓冲区大小\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4)#学习率\n",
    "    parser.add_argument(\n",
    "        \"--gamma\",#折扣因子\n",
    "        type=float,\n",
    "        default=0.95,\n",
    "        help=\"a smaller gamma favors earlier win\",\n",
    "    )\n",
    "    parser.add_argument(\"--n-step\", type=int, default=3)#\n",
    "    parser.add_argument(\"--target-update-freq\", type=int, default=320)#目标网络更新频率\n",
    "    parser.add_argument(\"--epoch\", type=int, default=50)#训练轮数\n",
    "    parser.add_argument(\"--num-episodes\", type=int, default=5000)\n",
    "    parser.add_argument(\"--step-per-epoch\", type=int, default=1000)#每轮训练的步数\n",
    "    parser.add_argument(\"--step-per-collect\", type=int, default=10)#每轮采集的步数\n",
    "    parser.add_argument(\"--update-per-step\", type=float, default=0.1)#每步更新的次数\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=64)#批大小\n",
    "    parser.add_argument(\"--hidden-sizes\", type=int, nargs=\"*\", default=[128, 128, 128, 128])#隐藏层大小\n",
    "    parser.add_argument(\"--training-num\", type=int, default=10)#训练环境数量\n",
    "    parser.add_argument(\"--test-num\", type=int, default=10)#测试环境数量\n",
    "    parser.add_argument(\"--logdir\", type=str, default=\"log\")#日志目录\n",
    "    parser.add_argument(\"--render\", type=float, default=0.1)#渲染频率\n",
    "    parser.add_argument(\"--tau\", type=float, default=1e-2)#软更新系数\n",
    "    parser.add_argument(\n",
    "        \"--win-rate\",#胜率\n",
    "        type=float,\n",
    "        default=0.6,\n",
    "        help=\"the expected winning rate: Optimal policy can get 0.7\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--watch\",#是否观察\n",
    "        default=True,\n",
    "        action=\"store_true\",\n",
    "        help=\"no training, watch the play of pre-trained models\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--agent-id\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"the learned agent plays as the agent_id-th player. Choices are 1 and 2.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume-path\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"the path of agent pth file for resuming from a pre-trained agent\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--opponent-path\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"the path of opponent agent pth file for resuming from a pre-trained agent\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        type=str,\n",
    "        default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "def get_args() -> argparse.Namespace:\n",
    "    parser = get_parser()\n",
    "    return parser.parse_known_args()[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T08:34:58.752179500Z",
     "start_time": "2024-03-06T08:34:58.727998800Z"
    }
   },
   "id": "609e344170a794bc"
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [],
   "source": [
    "#一系列工具函数\n",
    "import torch.nn.functional as F\n",
    "def onehot_from_logits(logits, eps=0.01):\n",
    "    ''' 生成最优动作的独热（one-hot）形式 '''\n",
    "    argmax_acs = (logits == logits.max(1, keepdim=True)[0]).float()\n",
    "    # 生成随机动作,转换成独热形式\n",
    "    rand_acs = torch.autograd.Variable(torch.eye(logits.shape[1])[[\n",
    "        np.random.choice(range(logits.shape[1]), size=logits.shape[0])\n",
    "    ]],\n",
    "                                       requires_grad=False).to(logits.device)\n",
    "    # 通过epsilon-贪婪算法来选择用哪个动作\n",
    "    return torch.stack([\n",
    "        argmax_acs[i] if r > eps else rand_acs[i]\n",
    "        for i, r in enumerate(torch.rand(logits.shape[0]))\n",
    "    ])\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20, tens_type=torch.FloatTensor):\n",
    "    \"\"\"从Gumbel(0,1)分布中采样\"\"\"\n",
    "    U = torch.autograd.Variable(tens_type(*shape).uniform_(),\n",
    "                                requires_grad=False)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    \"\"\" 从Gumbel-Softmax分布中采样\"\"\"\n",
    "    y = logits + sample_gumbel(logits.shape, tens_type=type(logits.data)).to(\n",
    "        logits.device)\n",
    "    return F.softmax(y / temperature, dim=1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1.0):\n",
    "    \"\"\"从Gumbel-Softmax分布中采样,并进行离散化\"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    y_hard = onehot_from_logits(y)\n",
    "    y = (y_hard.to(logits.device) - y).detach() + y\n",
    "    # 返回一个y_hard的独热量,但是它的梯度是y,我们既能够得到一个与环境交互的离散动作,又可以\n",
    "    # 正确地反传梯度\n",
    "    return y"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T08:34:58.753149500Z",
     "start_time": "2024-03-06T08:34:58.735516700Z"
    }
   },
   "id": "719766dc12be4c08"
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [],
   "source": [
    "class TwoLayerFC(torch.nn.Module):\n",
    "    def __init__(self, num_in, num_out, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_in, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T08:34:58.753149500Z",
     "start_time": "2024-03-06T08:34:58.741694700Z"
    }
   },
   "id": "2bcf2dca7b503115"
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    ''' DDPG算法 '''\n",
    "    def __init__(self, state_dim, action_dim, critic_input_dim, hidden_dim,\n",
    "                 actor_lr, critic_lr, device):\n",
    "        self.actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_actor = TwoLayerFC(state_dim, action_dim,\n",
    "                                       hidden_dim).to(device)\n",
    "        self.critic = TwoLayerFC(critic_input_dim, 1, hidden_dim).to(device)\n",
    "        self.target_critic = TwoLayerFC(critic_input_dim, 1,\n",
    "                                        hidden_dim).to(device)\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n",
    "                                                 lr=critic_lr)\n",
    "\n",
    "    def take_action(self, state, explore=False):\n",
    "        action = self.actor(state)\n",
    "        if explore:\n",
    "            action = gumbel_softmax(action)\n",
    "        else:\n",
    "            action = onehot_from_logits(action)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    def soft_update(self, net, target_net, tau):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) +\n",
    "                                    param.data * tau)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T08:34:58.753149500Z",
     "start_time": "2024-03-06T08:34:58.750703500Z"
    }
   },
   "id": "474d0ab7cf13c849"
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, env, device, actor_lr, critic_lr, hidden_dim,\n",
    "                 state_dims, action_dims, critic_input_dim, gamma, tau):\n",
    "        self.agents = []\n",
    "        for i in range(len(env.agents)):\n",
    "            self.agents.append(\n",
    "                DDPG(state_dims[i], action_dims[i], critic_input_dim,\n",
    "                     hidden_dim, actor_lr, critic_lr, device))\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.critic_criterion = torch.nn.MSELoss()\n",
    "        self.device = device\n",
    "\n",
    "    @property\n",
    "    def policies(self):\n",
    "        return [agt.actor for agt in self.agents]\n",
    "\n",
    "    @property\n",
    "    def target_policies(self):\n",
    "        return [agt.target_actor for agt in self.agents]\n",
    "\n",
    "    def take_action(self, states, explore):\n",
    "        states = [\n",
    "            torch.tensor([states[i]], dtype=torch.float, device=self.device)\n",
    "            for i in range(len(env.agents))\n",
    "        ]\n",
    "        return [\n",
    "            agent.take_action(state, explore)\n",
    "            for agent, state in zip(self.agents, states)\n",
    "        ]\n",
    "\n",
    "    def update(self, sample, i_agent):\n",
    "        obs, act, rew, next_obs, done = sample\n",
    "        cur_agent = self.agents[i_agent]\n",
    "\n",
    "        cur_agent.critic_optimizer.zero_grad()\n",
    "        all_target_act = [\n",
    "            onehot_from_logits(pi(_next_obs))\n",
    "            for pi, _next_obs in zip(self.target_policies, next_obs)\n",
    "        ]\n",
    "        target_critic_input = torch.cat((*next_obs, *all_target_act), dim=1)\n",
    "        target_critic_value = rew[i_agent].view(\n",
    "            -1, 1) + self.gamma * cur_agent.target_critic(\n",
    "                target_critic_input) * (1 - done[i_agent].view(-1, 1))\n",
    "        critic_input = torch.cat((*obs, *act), dim=1)\n",
    "        critic_value = cur_agent.critic(critic_input)\n",
    "        critic_loss = self.critic_criterion(critic_value,\n",
    "                                            target_critic_value.detach())\n",
    "        critic_loss.backward()\n",
    "        cur_agent.critic_optimizer.step()\n",
    "\n",
    "        cur_agent.actor_optimizer.zero_grad()\n",
    "        cur_actor_out = cur_agent.actor(obs[i_agent])\n",
    "        cur_act_vf_in = gumbel_softmax(cur_actor_out)\n",
    "        all_actor_acs = []\n",
    "        for i, (pi, _obs) in enumerate(zip(self.policies, obs)):\n",
    "            if i == i_agent:\n",
    "                all_actor_acs.append(cur_act_vf_in)\n",
    "            else:\n",
    "                all_actor_acs.append(onehot_from_logits(pi(_obs)))\n",
    "        vf_in = torch.cat((*obs, *all_actor_acs), dim=1)\n",
    "        actor_loss = -cur_agent.critic(vf_in).mean()\n",
    "        actor_loss += (cur_actor_out**2).mean() * 1e-3\n",
    "        actor_loss.backward()\n",
    "        cur_agent.actor_optimizer.step()\n",
    "\n",
    "    def update_all_targets(self):\n",
    "        for agt in self.agents:\n",
    "            agt.soft_update(agt.actor, agt.target_actor, self.tau)\n",
    "            agt.soft_update(agt.critic, agt.target_critic, self.tau)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T08:34:58.766202700Z",
     "start_time": "2024-03-06T08:34:58.760500800Z"
    }
   },
   "id": "72624742f55ae713"
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "outputs": [],
   "source": [
    "#定义环境\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "def get_env(render_mode: str | None = None):\n",
    "    return simple_adversary_v3.env()\n",
    "env=get_env(render_mode=\"human\")\n",
    "env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T08:35:32.233598800Z",
     "start_time": "2024-03-06T08:35:32.225869900Z"
    }
   },
   "id": "65d14c7622e3643b"
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "outputs": [
    {
     "data": {
      "text/plain": "(130,)"
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state=env.state()\n",
    "state.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T08:35:32.659944900Z",
     "start_time": "2024-03-06T08:35:32.650433400Z"
    }
   },
   "id": "9849add4669cd9b6"
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [
    {
     "data": {
      "text/plain": "([20, 22, 22, 22, 22, 22], [5, 5, 5, 5, 5, 5], 160)"
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "state_dims = []\n",
    "action_dims = []\n",
    "for action_space in env.action_spaces.values():\n",
    "    action_dims.append(action_space.n)\n",
    "for state_space in env.observation_spaces.values():\n",
    "    state_dims.append(state_space.shape[0])\n",
    "critic_input_dim = sum(state_dims) + sum(action_dims)\n",
    "state_dims, action_dims, critic_input_dim\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T08:35:54.600050100Z",
     "start_time": "2024-03-06T08:35:54.594328700Z"
    }
   },
   "id": "d92b0387e2643993"
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.18000619, -0.3794987 , -0.46482384, -0.840196  ,  1.2865343 ,\n",
      "       -0.19398841, -0.49590865, -0.8296097 ,  0.6696813 , -1.0392803 ,\n",
      "        0.90371025, -0.03025501,  0.57825226, -1.2179074 ,  1.0472523 ,\n",
      "       -0.31823295,  0.77298695, -0.6034404 ,  1.2229308 , -0.3207349 ],\n",
      "      dtype=float32), 0.0, False, False, {})\n"
     ]
    }
   ],
   "source": [
    "states=env.state()\n",
    "\n",
    "env.reset()\n",
    "print(env.last())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T08:35:56.913157600Z",
     "start_time": "2024-03-06T08:35:56.905540600Z"
    }
   },
   "id": "5b328fd7a75a02e4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "args=get_args()\n",
    "maddpg = MADDPG(env=env,\n",
    "                device=args.device,\n",
    "                actor_lr=args.lr, \n",
    "                critic_lr=args.lr, \n",
    "                hidden_dim=64,\n",
    "                state_dims=state_dims,\n",
    "                action_dims=action_dims,\n",
    "                critic_input_dim=critic_input_dim,\n",
    "                gamma=args.gamma, \n",
    "                tau=args.tau)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T08:34:58.792355200Z"
    }
   },
   "id": "a8eabd85fe2dd245"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_episodes = 5000\n",
    "episode_length = 25  # 每条序列的最大长度\n",
    "# buffer_size = 100000\n",
    "hidden_dim = 64\n",
    "actor_lr = 1e-2\n",
    "critic_lr = 1e-2\n",
    "gamma = 0.95\n",
    "tau = 1e-2\n",
    "batch_size = 1024\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "update_interval = 100\n",
    "minimal_size = 4000\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T08:34:58.793355600Z"
    }
   },
   "id": "1634338f09b5d6c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(env_id, maddpg, n_episode=10, episode_length=25):\n",
    "    # 对学习的策略进行评估,此时不会进行探索\n",
    "    env = get_env(render_mode=\"human\")\n",
    "    returns = np.zeros(len(env.agents))\n",
    "    for _ in range(n_episode):\n",
    "        obs = env.reset()\n",
    "        for t_i in range(episode_length):\n",
    "            actions = maddpg.take_action(obs, explore=False)\n",
    "            obs, rew, done, info = env.step(actions)\n",
    "            rew = np.array(rew)\n",
    "            returns += rew / n_episode\n",
    "    return returns.tolist()\n",
    "\n",
    "\n",
    "return_list = []  # 记录每一轮的回报（return）\n",
    "total_step = 0\n",
    "train_envs = DummyVectorEnv([get_env for _ in range(args.training_num)])\n",
    "test_envs = DummyVectorEnv([get_env for _ in range(args.test_num)])\n",
    "replay_buffer= VectorReplayBuffer(args.buffer_size,len(train_envs))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T08:34:58.794355700Z"
    }
   },
   "id": "37f3341ae88411eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state, reward, termination, truncation, info = env.last()\n",
    "    # ep_returns = np.zeros(len(env.agents))\n",
    "    print(env.last())\n",
    "    for e_i in range(episode_length):\n",
    "        actions = maddpg.take_action(state, explore=True)\n",
    "        next_state, reward, done, _ = env.step(actions)\n",
    "        replay_buffer.add(state, actions, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        total_step += 1\n",
    "        if replay_buffer.size(\n",
    "        ) >= minimal_size and total_step % update_interval == 0:\n",
    "            sample = replay_buffer.sample(batch_size)\n",
    "\n",
    "            def stack_array(x):\n",
    "                rearranged = [[sub_x[i] for sub_x in x]\n",
    "                              for i in range(len(x[0]))]\n",
    "                return [\n",
    "                    torch.FloatTensor(np.vstack(aa)).to(device)\n",
    "                    for aa in rearranged\n",
    "                ]\n",
    "\n",
    "            sample = [stack_array(x) for x in sample]\n",
    "            for a_i in range(len(env.agents)):\n",
    "                maddpg.update(sample, a_i)\n",
    "            maddpg.update_all_targets()\n",
    "    if (i_episode + 1) % 100 == 0:\n",
    "        ep_returns = evaluate(maddpg, n_episode=100)\n",
    "        return_list.append(ep_returns)\n",
    "        print(f\"Episode: {i_episode+1}, {ep_returns}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T08:34:58.795355100Z"
    }
   },
   "id": "b9a8ae98c8b693a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-06T08:34:58.796355100Z"
    }
   },
   "id": "b794da7f6ae8c9ef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
